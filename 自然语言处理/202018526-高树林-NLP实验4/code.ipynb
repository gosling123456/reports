{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1.导入必要的工具包"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import jieba\n",
    "import joblib\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Activation,Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.读取文件"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/pos.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-27-4c2294513cd1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mreview_data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[0mf_pos\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'data/pos.txt'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'r'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'utf8'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mf_pos\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'\\n'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/pos.txt'"
     ]
    }
   ],
   "source": [
    "def read_txt(path,label):\n",
    "    review_data = []\n",
    "    labels = []\n",
    "    f_pos = open(path, 'r', encoding='utf8')\n",
    "    for line in f_pos:\n",
    "        if line.strip('\\n') is not None:\n",
    "            tokens = list(jieba.cut(line))\n",
    "            processed_sent = \" \".join(tokens)\n",
    "            review_data.append(processed_sent)\n",
    "            labels.append(label)\n",
    "    return review_data\n",
    "\n",
    "f_pos = open('data/pos.txt', 'r', encoding='utf8')\n",
    "for line in f_pos:\n",
    "    if line.strip('\\n') is not None:\n",
    "        tokens = list(jieba.cut(line))\n",
    "        processed_sent = \" \".join(tokens)\n",
    "        review_data.append(processed_sent)\n",
    "        labels.append(1)\n",
    "print(len(label))\n",
    "\n",
    "# f_neg = open('data/neg.txt', 'r', encoding='utf8')\n",
    "# for line in f_neg:\n",
    "#     if line.strip('\\n') is not None:\n",
    "#         tokens = list(jieba.cut(line))\n",
    "#         processed_sent = \" \".join(tokens)\n",
    "#         review_data.append(processed_sent)\n",
    "#         labels.append(0)\n",
    "# print(len(review_data))\n",
    "# print(review_data[3], labels[3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# avglen = 0 #句子最大长度\n",
    "# len_list=[]\n",
    "# for sent_str in review_data:\n",
    "#     words=list(jieba.cut(sent_str))\n",
    "#     length = len(words)\n",
    "#     len_list.append(length)\n",
    "#     avglen=np.sum(np.array(len_list))/len(len_list)\n",
    "# print('avg_len:',avglen)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.文本向量化"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def txt_vectorize(text):\n",
    "    tokenizer = Tokenizer() # 创建一个Tokenizer对象，将一个词转换为正整数\n",
    "    tokenizer.fit_on_texts(text) #将词编号，词频越大，编号越小\n",
    "    word2index = tokenizer.word_index\n",
    "    # vocab_size=len(word2index)\n",
    "    # index2word = {word2index[word]:word for word in word2index}\n",
    "    x_word_ids = tokenizer.texts_to_sequences(text) #将句子中的每个词转换为数字\n",
    "    x_padded_seqs = np.array(pad_sequences(x_word_ids,truncating='post',maxlen=100))#将每个句子设置为\n",
    "    return x_padded_seqs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 创建深度学习模型， Embedding + LSTM + Softmax.\n",
    "def create_LSTM(n_units, input_size, output_dim,vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size + 1, output_dim=output_dim,\n",
    "    input_length=input_size, mask_zero=True))\n",
    "    model.add(LSTM(n_units, input_shape=(None,input_size)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_dim , activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.划分数据集和测试集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-25-7fee7b28b606>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m         \u001B[0my_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels1\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# 打乱结果转换成列表\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mx_test\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m \u001B[0mx_padded_seqs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtxt_vectorize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m \u001B[1;31m# print(len(x_padded_seqs))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mx_test\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msplit_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_padded_seqs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m)\u001B[0m     \u001B[1;31m# 测试集+训练集占比0.2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "def split_data(x_padded_seqs,labels,K):  # 划分数据集和测试集\n",
    "    if 0 < K <= 1:\n",
    "        c = list(zip(x_padded_seqs,labels))\n",
    "        shuffle(c) # 打乱顺序\n",
    "        x_padded_seqs,labels, = zip(*c)\n",
    "        x_padded_seqs, labels = list(x_padded_seqs)[0:int(len(x_padded_seqs)*K)],list(labels)[0:int(len(x_padded_seqs)*K)]\n",
    "        return train_test_split(x_padded_seqs,labels,test_size=0.2,shuffle=0)\n",
    "    else:\n",
    "        x_padded_seqs = x_padded_seqs.tolist()\n",
    "        c = list(zip(x_padded_seqs[0:K]+x_padded_seqs[-1900:],labels[0:K]+labels[-1900:]))  # 测试集\n",
    "        d = list(zip(x_padded_seqs[K:-1900],labels[K:-1900]))                                 # 验证集\n",
    "        shuffle(c)\n",
    "        shuffle(d)\n",
    "        x_padded_seqs,labels, = zip(*c)\n",
    "        x_train = list(x_padded_seqs)\n",
    "        y_train = list(labels)  # 打乱结果转换成列表\n",
    "        x_padded_seqs1,labels1, = zip(*d)\n",
    "        x_test = list(x_padded_seqs1)\n",
    "        y_test = list(labels1)  # 打乱结果转换成列表\n",
    "        return x_train,x_test,y_train,y_test\n",
    "# x_padded_seqs = txt_vectorize(text)\n",
    "# print(len(x_padded_seqs))\n",
    "x_train,x_test,y_train,y_test=split_data(x_padded_seqs,labels,0.2)     # 测试集+训练集占比0.2\n",
    "# x_train,x_test,y_train,y_test=split_data(x_padded_seqs,labels,0.5)     # 测试集+训练集占比0.5\n",
    "# x_train,x_test,y_train,y_test=split_data(x_padded_seqs,labels,1)       # 测试集+训练集占比1\n",
    "# x_train,x_test,y_train,y_test=split_data(x_padded_seqs,labels,860)     # 训练集+训练集数量为1900+860\n",
    "# x_train,x_test,y_train,y_test=split_data(x_padded_seqs,labels,500)     # 训练集数量为1900+500\n",
    "# x_train,x_test,y_train,y_test=split_data(x_padded_seqs,labels,100)     # 训练集数量为1900+100\n",
    "\n",
    "#\n",
    "# K = [0.2,0.5,1,860,500,100]\n",
    "# for i in K:\n",
    "#     x_train,x_test,y_train,y_test=split_data(x_padded_seqs,labels,i)\n",
    "#     print(len(x_train))\n",
    "#     print(len(x_test))\n",
    "#     print(len(y_train))\n",
    "#     print(len(y_test))\n",
    "#     x_train=np.array(x_train)\n",
    "#     x_test=np.array(x_test)\n",
    "#     y_train=np.array(y_train)\n",
    "#     y_test=np.array(y_test)\n",
    "#     y_train_onehot=to_categorical(y_train)\n",
    "#     y_test_hot=to_categorical(y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.训练并保存模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def model_train(name):\n",
    "    # 模型输入参数，需要自己根据需要调整\n",
    "    input_size = txt_vectorize[1]\n",
    "    print(input_size)\n",
    "    n_units = 100\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    output_dim = 2\n",
    "    # 模型训练\n",
    "    lstm_model = create_LSTM(n_units, input_size, output_dim,vocab_size=vocab_size)\n",
    "    lstm_model.fit(x_train, y_train_onehot, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    # joblib.dump(lstm_model,\"models/Precision@\"+name+'.model')\n",
    "    results=lstm_model.predict(x_test)\n",
    "    result_labels = np.argmax(results, axis=-1) # 获得最大概率对应的标签\n",
    "    # print(type(result_labels))\n",
    "    print('准确率', accuracy_score(y_test, result_labels))\n",
    "\n",
    "    Precision = []\n",
    "    correct = 0\n",
    "    for i in range(len(result_labels)):\n",
    "        if result_labels[i] == y_test[i]:\n",
    "            correct += 1\n",
    "        Precision.append(correct / (i + 1))\n",
    "    num = [i+1 for i in range(len(result_labels))]\n",
    "    plt.plot(num, Precision, c='red')\n",
    "    # plt.scatter(num,Precision, c='red')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.xlabel(\"累计测试样本个数\", fontdict={'size': 16})\n",
    "    plt.ylabel(\"实时准确率\", fontdict={'size': 16})\n",
    "    plt.title(\"Precision@\"+name, fontdict={'size': 20})\n",
    "    plt.text(x=num[-1],y=Precision[-1],s='ACC:%.2f'%Precision[-1],fontsize=15)\n",
    "    plt.savefig('figures/Precision@'+name+'.jpg')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.使用模型预测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# lstm_model = model_train()\n",
    "# results=lstm_model.predict(x_test)\n",
    "# result_labels = np.argmax(results, axis=-1) # 获得最大概率对应的标签\n",
    "# # print(type(result_labels))\n",
    "# print('准确率', accuracy_score(y_test, result_labels))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.样本预测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Precision = []\n",
    "# correct = 0\n",
    "# for i in range(len(result_labels)):\n",
    "#     if result_labels[i] == y_test[i]:\n",
    "#         correct += 1\n",
    "#     Precision.append(correct / (i + 1))\n",
    "# num = [i+1 for i in range(len(result_labels))]\n",
    "# plt.plot(num, Precision, c='red')\n",
    "# # plt.scatter(num,Precision, c='red')\n",
    "# plt.grid(True, linestyle='--', alpha=0.5)\n",
    "# plt.xlabel(\"累计测试样本个数\", fontdict={'size': 16})\n",
    "# plt.ylabel(\"实时准确率\", fontdict={'size': 16})\n",
    "# plt.title('Precision', fontdict={'size': 20})\n",
    "# plt.savefig('Precision.jpg')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# mark = ['差评','好评']\n",
    "# new_reviews=['体验不是很好，信号差还发烫。手机第一次充电就发烫的要死。热点总是自动打开','东西非常好，用起来也很方便，总之就很棒']\n",
    "# new_sents=[]\n",
    "# for sent_str in new_reviews:\n",
    "#     tokens=jieba.cut(sent_str)\n",
    "#     sent=' '.join(tokens)\n",
    "#     new_sents.append(sent)\n",
    "# x_new_ids = tokenizer.texts_to_sequences(new_sents) #将句子中的每个词转换为数字\n",
    "# # print(x_new_ids[0])\n",
    "# x_new_padseqs = pad_sequences(x_new_ids,truncating='post',maxlen=100)#将每个句子设置为\n",
    "# # print(x_new_padseqs)\n",
    "# # print(type(x_new_ids))\n",
    "# probs=lstm_model.predict(x_new_padseqs)\n",
    "# new_labels=np.argmax(probs,axis=-1)\n",
    "# print(new_labels)\n",
    "# for i in range(new_labels.shape[0]):\n",
    "#     print(mark[new_labels[i]],end=' ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_padded_seqs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-23-ebfc134f32c1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mname\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m'N+P=20%'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'N+P=50%'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'N+P=100%'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'N=1900,P=860'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'N=1900,P=500'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'N=1900,P=100'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m6\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m         \u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mx_test\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msplit_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_padded_seqs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mK\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'x_padded_seqs' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    K = [0.2,0.5,1,860,500,100]\n",
    "    name = ['N+P=20%','N+P=50%','N+P=100%','N=1900,P=860','N=1900,P=500','N=1900,P=100']\n",
    "    for i in range(6):\n",
    "        x_train,x_test,y_train,y_test=split_data(x_padded_seqs,labels,K[i])\n",
    "        print(len(x_train))\n",
    "        print(len(x_test))\n",
    "        print(len(y_train))\n",
    "        print(len(y_test))\n",
    "        x_train=np.array(x_train)\n",
    "        x_test=np.array(x_test)\n",
    "        y_train=np.array(y_train)\n",
    "        y_test=np.array(y_test)\n",
    "        y_train_onehot=to_categorical(y_train)\n",
    "        y_test_hot=to_categorical(y_test)\n",
    "        model_train(name[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
